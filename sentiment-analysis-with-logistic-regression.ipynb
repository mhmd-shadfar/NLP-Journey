{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMq9TVbQpmmdmcx8P89oN78",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshojaei77/NLP-Journey/blob/main/01_sentiment-analysis-with-logistic-regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install & Import Libraries\n",
        "\n",
        "First, let's import all the necessary libraries.\n"
      ],
      "metadata": {
        "id": "KpQOfIlqDjQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nltk scikit-learn transformers datasets mlflow"
      ],
      "metadata": {
        "id": "ND4AXelkI5JL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N3VLT_ciDfCF"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from datasets import load_dataset\n",
        "from torch.cuda.amp import autocast"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Download NLTK Data\n",
        "\n",
        "NLTK (Natural Language Toolkit) is a popular library for natural language processing. We need to download some data that NLTK uses for tokenization and stopwords removal.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "umhU4qJ_EeN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4w_9J5oEX3a",
        "outputId": "e2100c0e-78dd-462f-a074-ccef93db8e47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`nltk.download('punkt')`**: This command downloads the Punkt Tokenizer Models. Punkt is a pre-trained unsupervised machine learning model for tokenizing text into sentences. It's useful for breaking up text into individual sentences, which is a crucial preprocessing step in many NLP tasks.\n",
        "\n",
        "2. **`nltk.download('stopwords')`**: This command downloads the NLTK stopwords corpus. Stop words are common words in a language (like 'the', 'is', 'in') that typically do not contain important meaning and are often removed from texts during preprocessing to reduce noise and focus on words that carry more significant meaning. NLTK provides a list of stop words for several languages, which can be used to filter out these common words from textual data.\n",
        "\n",
        "3. **`nltk.download('wordnet')`**: This command downloads WordNet, a large lexical database of English words. WordNet groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members. WordNet can be used for word sense disambiguation, text analysis, and natural language understanding tasks."
      ],
      "metadata": {
        "id": "4z2PkrDxFzYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Load IMDB Dataset\n",
        "We will load the IMDB dataset from Hugging Face's datasets library."
      ],
      "metadata": {
        "id": "fn-xa2mkEqk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('imdb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hQljC3zEpI3",
        "outputId": "84438939-d465-4cba-9ecb-3b9b8761f938"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Preprocess the Data\n",
        "\n",
        "We need to clean and prepare the text data before feeding it into the model. Stop words are common words like 'the', 'is', 'and' that do not carry much meaning."
      ],
      "metadata": {
        "id": "3G9kBGvDErH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Extract texts and labels from the dataset\n",
        "texts = [preprocess(sample['text']) for sample in dataset['train']]\n",
        "labels = [sample['label'] for sample in dataset['train']]"
      ],
      "metadata": {
        "id": "gB-unFLpErnM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Text Embeddings using BERT\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained model that provides rich contextual embeddings. We will use BERT to convert the text data into embeddings. Ensure that the model and data are loaded onto the GPU.\n"
      ],
      "metadata": {
        "id": "SRdM8ntiEr9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\").to('cuda')\n",
        "\n",
        "def get_embeddings(texts):\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        with autocast():\n",
        "            outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "    return embeddings\n",
        "\n",
        "def get_embeddings_in_batches(texts, batch_size=32):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        embeddings = get_embeddings(batch)\n",
        "        all_embeddings.append(embeddings)\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "X_embeddings = get_embeddings_in_batches(texts)"
      ],
      "metadata": {
        "id": "WWx5ClS0Esl8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Split the Data into Training and Testing Sets\n",
        "\n",
        "We need to split our data into a training set and a testing set to evaluate our model's performance. `train_test_split` splits the data randomly, with 80% for training and 20% for testing."
      ],
      "metadata": {
        "id": "Sxmh2v6LEuUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "EqZTwTqLEuv9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Define a Pipeline with StandardScaler and Logistic Regression\n",
        "\n",
        "A pipeline allows us to chain multiple steps together, making the process more efficient. `StandardScaler` standardizes the features by removing the mean and scaling to unit variance. Logistic Regression is a simple yet powerful classification algorithm."
      ],
      "metadata": {
        "id": "sS7zVtAIEu_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(penalty='l2', solver='liblinear', class_weight='balanced'))\n",
        "])"
      ],
      "metadata": {
        "id": "-sZoOlh2EvVI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Start an MLflow Run\n",
        "\n",
        "MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It helps in tracking experiments, packaging code into reproducible runs, and sharing and deploying models."
      ],
      "metadata": {
        "id": "ur70oOkxEvrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run():\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.log_param(\"model\", \"LogisticRegression with BERT Embeddings\")\n",
        "    mlflow.sklearn.log_model(pipeline, \"logistic_regression_bert_model\")\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "    print(\"Confusion Matrix:\\n\", conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuBTLSGAEwLR",
        "outputId": "1883e566-4410-4614-b8af-7775153b3b47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:32: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
            "  warnings.warn(\n",
            "2024/08/21 09:23:47 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8328\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83      2515\n",
            "           1       0.83      0.83      0.83      2485\n",
            "\n",
            "    accuracy                           0.83      5000\n",
            "   macro avg       0.83      0.83      0.83      5000\n",
            "weighted avg       0.83      0.83      0.83      5000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[2099  416]\n",
            " [ 420 2065]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Example Prediction\n",
        "\n",
        "Demonstrate how to make a prediction on new text data."
      ],
      "metadata": {
        "id": "gSYMDiVPFCcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = [\"This movie was absolutely fantastic!\"]\n",
        "new_text_processed = [preprocess(text) for text in new_text]\n",
        "new_text_embeddings = get_embeddings(new_text_processed)\n",
        "prediction = pipeline.predict(new_text_embeddings)\n",
        "\n",
        "# Mapping the prediction to a human-readable sentiment\n",
        "sentiment_mapping = {0: \"negative\", 1: \"positive\"}\n",
        "predicted_sentiment = sentiment_mapping[prediction[0]]\n",
        "\n",
        "print(\"The sentiment of the text '{}' is predicted to be: {}\".format(new_text[0], predicted_sentiment))\n",
        "\n",
        "# Additional explanation for beginners\n",
        "if predicted_sentiment == \"positive\":\n",
        "    print(\"This means the model thinks the text expresses a positive opinion or feeling.\")\n",
        "else:\n",
        "    print(\"This means the model thinks the text expresses a negative opinion or feeling.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F916TCtWE_d1",
        "outputId": "cfd6731e-d05e-492e-c4a7-427c578b9028"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the text 'This movie was absolutely fantastic!' is predicted to be: positive\n",
            "This means the model thinks the text expresses a positive opinion or feeling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "This Jupyter Notebook guides you through the process of building a sentiment analysis model using logistic regression and BERT embeddings on the IMDB dataset. Each step is explained in detail to help you understand the purpose and functionality of every part of the code. The model is evaluated using accuracy, a classification report, and a confusion matrix, and the results are logged with MLflow for tracking and reproducibility.\n",
        "\n"
      ],
      "metadata": {
        "id": "qkDBWcQbFCz4"
      }
    }
  ]
}
