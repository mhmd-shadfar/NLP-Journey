# NLP Journey - Roadmap to Learn LLMs from Scratch with Modern NLP Methods in 2024

This repository provides a comprehensive guide for learning Natural Language Processing (NLP) from the ground up, progressing to the understanding and application of Large Language Models (LLMs). It focuses on practical skills needed for NLP and LLM-related roles in 2024 and beyond.  We'll leverage Jupyter Notebooks for hands-on practice.

**Note:** The resources listed below include text-based tutorials, academic papers, GitHub repositories, and (where available) Jupyter Notebooks.

## Chapter 1: Foundations of NLP

#### Core NLP Concepts

| Topic                                       | Resources                                           | 
|---------------------------------------------|------------------------------------------------------|
| Introduction to NLP: Syntax, Semantics, Pragmatics, Discourse                      | [What is Natural Language Processing (NLP)?)](https://www.datacamp.com/blog/what-is-natural-language-processing) | 

**Chapter 2: Essential NLP Tasks & Algorithms**

| Topic                                                    | Resources                                                                                          | Papers           | Practices 
|----------------------------------------------------------|----------------------------------------------------------------------------------------------------|-----------|-----------|
| Text Classification                                      | [Scikit-learn Text Classification](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html), [Hugging Face Text Classification](https://huggingface.co/docs/transformers/tasks/sequence_classification), [FastText](https://github.com/facebookresearch/fastText) |  **2018:** [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805v2) |  [] 
| Sentiment Analysis                                      | [NLTK Sentiment Analysis](https://www.nltk.org/howto/sentiment.html), [TextBlob Sentiment Analysis](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis), [VADER Sentiment Analysis](https://github.com/cjhutto/vaderSentiment) | **2019:** [Zero-shot Text Classification With Generative Language Models](https://arxiv.org/abs/1912.10165v1) |  [] 
| Named Entity Recognition (NER)                          | [NLTK NER](https://www.nltk.org/book/ch07.html), [spaCy NER](https://spacy.io/usage/linguistic-features#named-entities), [Hugging Face NER](https://huggingface.co/docs/transformers/tasks/token-classification), [MIT Information Extraction Toolkit](https://github.com/mit-nlp/MITIE) | **2021:** [PoKE: A Prompt-based Knowledge Eliciting Approach for Event Argument Extraction](https://arxiv.org/abs/2109.05190v3) |  []
| Information Retrieval                                     | [Elasticsearch](https://www.elastic.co/), [Solr](https://lucene.apache.org/solr/), [Pinecone](https://www.pinecone.io/) | **2020:** [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832v2)<br> **2021:** [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://arxiv.org/abs/2112.01488v3) |  [] 
| Question Answering                                       | [DrQA](https://github.com/facebookresearch/DrQA), [Document-QA](https://github.com/allenai/document-qa)  | **2021:** [General-Purpose Question-Answering with Macaw](https://arxiv.org/abs/2109.02593v1)<br>**2021:** [WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332v3)  | []

**Chapter 3: Deep Learning for NLP**

| Topic                                                         | Resources                                                                                                                                                                             | Papers           | Practices 
|--------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| Recurrent Neural Networks (RNNs)                              | [colah's blog: Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), [Andrej Karpathy: The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), [Bayesian Recurrent Neural Network for Language Modeling](http://chien.cm.nctu.edu.tw/bayesian-recurrent-neural-network-for-language-modeling/), [RNNLM](http://www.fit.vutbr.cz/~imikolov/rnnlm/),  [KALDI LSTM](https://github.com/dophist/kaldi-lstm)  | **2019:** [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509v1) |  []
| Deep Learning Frameworks                                     | [PyTorch Tutorials](https://pytorch.org/tutorials/), [JAX Documentation](https://jax.readthedocs.io/en/latest/), [TensorFlow Tutorials](https://www.tensorflow.org/tutorials), [Caffe](http://arxiv.org/pdf/1409.3215v1.pdf)  | (None - this is about frameworks) |  []

**Chapter 4: Large Language Models (LLMs)**

* **The Transformer Architecture**

| Topic                                                            | Resources                                                                                                         | Papers           | Practices 
|-----------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| Attention, Residual Connections, Layer Normalization, RoPE        | [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/), [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/), [Visual Intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M&t=187s), [LLM Visualization](https://bbycroft.net/llm), [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY), [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/) | **2017**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762v7) | []

* **LLM Architectures, Pre-training, & Post-training** 

| Topic                                                                       | Resources                                                                                                                                                                                                                    | Papers           | Practices 
|---------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| GPT, BERT, T5, Llama, PaLM, Phi-3                                         | [LLMDataHub](https://github.com/Zjh-819/LLMDataHub), [Hugging Face: Causal Language Modeling](https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt), [TinyLlama](https://github.com/jzhang38/TinyLlama), [Chinchilla's Wild Implications](www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications), [BLOOM](https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4), [OPT-175 Logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf), [LLM 360](https://www.llm360.ai/), [New LLM Pre-training and Post-training Paradigms](https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training), [Phi-3CookBook](https://github.com/microsoft/Phi-3CookBook), [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373), [LLM Reading List](https://github.com/crazyofapple/Reading_groups/),  [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752), [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434),  [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/pdf/2403.19887), [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060), [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783) | **2018**: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805v2) <br>**2019**: [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461v1) <br>**2019**: [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053v4) <br> **2020**: [The Pile: An 800GB Dataset of Diverse Text for Language Modeling](https://arxiv.org/abs/2101.00027v1)<br>**2020**: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150v2) <br> **2020**: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)<br>**2021**: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602v3) <br> **2021**: [UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v3)<br>**2022**: [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682v2)<br>**2022**: [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085) <br>**2023**: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1)<br> **2023**: [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045v2)<br> **2023**: [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564v3) <br> **2023**: [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644v2) <br> **2023**: [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463v1) <br>**2024**: [LLaMA Pro: Progressive LLaMA with Block Expansion](https://arxiv.org/abs/2401.02415v1)<br> **2024**: [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954v1) <br> **2024**: [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)<br>**2024**: [Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159v2) <br>**2024**: [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385v1) <br>**2024**: [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) <br> **2024**: [OpenELM: An Efficient Language Model Family with Open Training and Inference Framework](https://arxiv.org/abs/2404.14619)  |  [] 
| Emerging Architectures (DeepSeek-v2, Jamba, Mixture of Experts - MoE) | [DeepSeek-v2](https://arxiv.org/abs/2405.04434), [Jamba](https://arxiv.org/abs/2403.19887), [Hugging Face: Mixture of Experts Explained](https://huggingface.co/blog/moe), [Create MoEs with MergeKit Notebook](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing),  [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/pdf/2112.06905.pdf), [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf) | **2023**: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752)<br>**2024**: [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/pdf/2403.19887)<br>**2024**: [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060)<br>**2024**: [Mixture of A Million Experts](https://arxiv.org/abs/2407.04153v1) <br>**2024**: [OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2402.01739)  |  [] 

* **Fine-tuning & Adapting LLMs**

| Topic                                                                    | Resources                                                                                                                                                                                               | Papers           | Practices 
|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| Supervised Fine-Tuning (SFT)                                             | [Fine-Tune Your Own Llama 2 Model](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html), [Padding Large Language Models](https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff), [A Beginner's Guide to LLM Fine-Tuning](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html), [unslothai](https://github.com/unslothai/unsloth), [Fine-tune Llama 2 with QLoRA Notebook](https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing), [Fine-tune CodeLlama using Axolotl Notebook](https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing), [Fine-tune Mistral-7b with QLoRA Notebook](https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS?usp=sharing), [Fine-tune Mistral-7b with DPO Notebook](https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing), [Fine-tune Llama 3 with ORPO Notebook](https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi), [Fine-tune Llama 3.1 with Unsloth Notebook](https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z?usp=sharing), [flux-finetune](https://github.com/gradient-ai/flux-finetune), [torchtune](https://github.com/pytorch/torchtune), [Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/pdf/2301.13688.pdf) | **2020**: [Making Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2012.15723v2) <br>**2021**: [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652v5) <br> **2022**: [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/abs/2212.12017) <br>**2023**: [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688) <br> **2024**: [Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model](https://arxiv.org/abs/2402.07827)<br>**2024**: [Instruction Pre-Training: Language Models are Supervised Multitask Learners](https://arxiv.org/abs/2406.14491v1) | [] 
| Parameter-Efficient Fine-tuning (PEFT) (LoRA, Adapters, Prompt Tuning)  | [LoRA Insights](https://lightning.ai/pages/community/lora-insights/), [Hugging Face: Parameter-Efficient Fine-Tuning](https://huggingface.co/blog/peft), [FLAN](https://openreview.net/forum?id=gEZrGCozdqR), [T0](https://arxiv.org/abs/2110.08207)                                                  | **2021**: [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190v1) <br>**2021**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685v2)<br>**2021**: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691v2) <br>**2021**: [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://arxiv.org/abs/2109.04332v3) <br>**2021**: [PTR: Prompt Tuning with Rules for Text Classification](https://arxiv.org/abs/2105.11259v3)<br>**2021**: [Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification](https://arxiv.org/abs/2108.02035v2)<br>**2022**: [KronA: Parameter Efficient Tuning with Kronecker Adapter](https://arxiv.org/abs/2212.10650v1) <br> **2023**: [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199v2)<br> **2023**: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861v1) <br>**2023**: [AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512v2)<br>**2024**: [ResLoRA: Identity Residual Mapping in Low-Rank Adaption](https://arxiv.org/abs/2402.18039) <br>**2024**: [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) <br> **2024**: [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507)<br>**2024**: [S-LoRA: Serving Thousands of Concurrent LoRA Adapters](https://arxiv.org/abs/2311.03285v2) <br> **2024**: [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659v4) | [] 
| Reinforcement Learning from Human Feedback (RLHF) (PPO, DPO)              | [Distilabel](https://github.com/argilla-io/distilabel), [An Introduction to Training LLMs using RLHF](https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy), [Hugging Face: Illustration RLHF](https://huggingface.co/blog/rlhf), [Hugging Face: Preference Tuning LLMs](https://huggingface.co/blog/pref-tuning), [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives), [Fine-tune Mistral-7b with DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html), [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf), [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf), [WebGPT: Browser-assisted question-answering with human feedback](www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22), [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/pdf/2209.14375.pdf), [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/pdf/2212.12017) | **2021**: [WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332v3) <br> **2022**: [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155v1)<br>**2022**: [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073v1)<br>**2023**: [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) <br>**2024**: [HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback](https://arxiv.org/abs/2403.08309v2) <br>**2024**: [RLHF Workflow: From Reward Modeling to Online RLHF](https://arxiv.org/abs/2405.07863v1) |  [] 
| Model Merging (SLERP, DARE/TIES, FrankenMoEs)                              | [Merge LLMs with mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html), [DARE/TIES](https://arxiv.org/abs/2311.03099), [Phixtral](https://huggingface.com/mlabonne/phixtral-2x2_8), [MergeKit](https://github.com/cg123/mergekit), [Merge LLMs with MergeKit Notebook](https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing), [LazyMergekit Notebook](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing) | **2023**: [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://arxiv.org/abs/2311.03099v1) <br> **2024**: [Knowledge Fusion of Large Language Models](https://arxiv.org/abs/2401.10491) <br> **2024**: [FuseChat: Knowledge Fusion of Chat Models](https://arxiv.org/abs/2408.07990v1) <br>**2024**: [Evolutionary Optimization of Model Merging Recipes](https://arxiv.org/abs/2403.13187) | [] 

* **LLM Evaluation**

| Topic                                              | Resources                                                                                                                                                                                   | Papers           | Practices 
|---------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| LLM Evaluation Benchmarks & Tools                  | [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [MixEval](https://github.com/Psycoy/MixEval), [lighteval](https://github.com/huggingface/lighteval), [OLMO-eval](https://github.com/allenai/OLMo-Eval), [instruct-eval](https://github.com/declare-lab/instruct-eval), [simple-evals](https://github.com/openai/simple-evals), [Giskard](https://github.com/Giskard-AI/giskard), [LangSmith](https://www.langchain.com/langsmith),  [Ragas](https://github.com/explodinggradients/ragas), [Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard), [MixEval Leaderboard](https://mixeval.github.io/#leaderboard), [AlpacaEval Leaderboard](https://tatsu-lab.github.io/alpaca_eval/), [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), [OpenCompass 2.0 LLM Leaderboard](https://rank.opencompass.org.cn/leaderboard-llm-v2), [Berkeley Function-Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html), [HELM](https://arxiv.org/pdf/2211.09110.pdf), [BIG-bench](https://github.com/google/BIG-bench) | **2021**: [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958) <br> **2022**: [Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110) <br> **2023**: [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)<br>**2024**: [Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference](https://arxiv.org/abs/2403.04132) <br>**2024**: [LLMEval: A Preliminary Study on How to Evaluate Large Language Models](https://arxiv.org/abs/2312.07398v2) <br>**2024**: [PromptBench: A Unified Library for Evaluation of Large Language Models](https://arxiv.org/abs/2312.07910v2) <br> **2024**: [Catwalk: A Unified Language Model Evaluation Framework for Many Datasets](https://arxiv.org/abs/2312.10253v1) <br>**2024**: [Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs](https://arxiv.org/abs/2312.17080v1) | []  

Okay, here are the remaining chapters with the papers added:

**Chapter 4: Large Language Models (LLMs) - Continued**

* **Prompt Engineering** (Continued)

| Topic                                                    | Resources                                                                                                                                       | Papers           | Practices 
|----------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| Prompt Engineering Techniques (Zero-Shot, Few-Shot, Chain-of-Thought, ReAct) (Continued)  |  |**2021**: [Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification](https://arxiv.org/abs/2108.02035v2)<br>**2021**: [Noisy Channel Language Model Prompting for Few-Shot Text Classification](https://arxiv.org/abs/2108.04106v3)<br>**2021**: [Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/abs/2108.13161v7)<br>**2021**: [Want To Reduce Labeling Cost? GPT-3 Can Help](https://arxiv.org/abs/2108.13487v1)<br>**2021**: [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/abs/2109.01247v2)<br>**2021**: [Discrete and Soft Prompting for Multilingual Models](https://arxiv.org/abs/2109.03630v1)<br>**2021**: [Open Aspect Target Sentiment Classification with Natural Language Prompts](https://arxiv.org/abs/2109.03685v1)<br>**2021**: [Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning](https://arxiv.org/abs/2109.04144v1)<br>**2021**: [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://arxiv.org/abs/2109.04332v3)<br>**2021**: [CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems](https://arxiv.org/abs/2109.04645v4)<br>**2021**: [PoKE: A Prompt-based Knowledge Eliciting Approach for Event Argument Extraction](https://arxiv.org/abs/2109.05190v3)<br>**2021**: [Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation](https://arxiv.org/abs/2109.06513v2)<br>**2021**: [Can Language Models be Biomedical Knowledge Bases?](https://arxiv.org/abs/2109.07154v1)<br>**2021**: [Language Models are Few-shot Multilingual Learners](https://arxiv.org/abs/2109.07684v1)<br>**2021**: [Reframing Instructional Prompts to GPTk's Language](https://arxiv.org/abs/2109.07830v3)<br>**2021**: [SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2109.08306v1)<br> **2021**: [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387v3)<br>**2021**: [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207v3)<br>**2022**: [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903v6) <br>**2022**: [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171v4)<br>**2022**: [Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango](https://arxiv.org/abs/2209.07686v2)<br>**2022**: [Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493v1)<br> **2022**: [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://arxiv.org/abs/2209.09513v2)<br>**2022**: [Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks](https://arxiv.org/abs/2211.12588v4) <br>**2022**: [Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters](https://arxiv.org/abs/2212.10001v2)<br>**2023**: [Faithful Chain-of-Thought Reasoning](https://arxiv.org/abs/2301.13379v3) <br> **2023**: [Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093v3) <br>**2023**: [Active Prompting with Chain-of-Thought for Large Language Models](https://arxiv.org/abs/2302.12246v3)<br> **2023**: [Guiding Large Language Models via Directional Stimulus Prompting](https://arxiv.org/abs/2302.11520v4)<br>**2023**: [Reasoning Implicit Sentiment with Chain-of-Thought Prompting](https://arxiv.org/abs/2305.11255v4)<br> **2023**: [Language Models are Multilingual Chain-of-Thought Reasoners](https://arxiv.org/abs/2210.03057v1) <br>**2023**: [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629v3) <br> **2023**: [From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting](https://arxiv.org/abs/2309.04269v1)<br>**2024**: [Chain-of-Thought Prompting Under Streaming Batch: A Case Study](https://arxiv.org/abs/2306.00550v1) <br>**2024**: [The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models](https://arxiv.org/abs/2401.05618v1)<br>**2024**: [The Impact of Reasoning Step Length on Large Language Models](https://arxiv.org/abs/2401.04925v2)<br>**2024**: [Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding](https://arxiv.org/abs/2401.04398v1) <br>**2024**: [CLLMs: Consistency Large Language Models](https://arxiv.org/abs/2403.00835v3)<br>**2024**: [RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation](https://arxiv.org/abs/2403.05313) <br>**2024**: [In-Context Principle Learning from Mistakes](https://arxiv.org/abs/2402.05403) <br>**2024**: [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs/2310.06117v1) <br> **2024**: [Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers Faster](https://arxiv.org/abs/2311.08263v1) <br> **2024**: [Contrastive Chain-of-Thought Prompting](https://arxiv.org/abs/2311.09277v1) | []
| Task-Specific Prompting (e.g., Code Generation)         | [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](https://arxiv.org/abs/2401.08500), [Codex](https://arxiv.org/abs/2107.03374.pdf)                              |**2022**: [Language Models of Code are Few-Shot Commonsense Learners](https://arxiv.org/abs/2210.07128v3) <br> **2022**: [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435v2) <br> **2023**: [LLM4Decompile: Decompiling Binary Code with Large Language Models](https://arxiv.org/abs/2403.05286v1)  | []
| Structuring LLM Outputs (Templates, JSON, LMQL, Outlines, Guidance) | [Chat Template](https://huggingface.co/blog/chat-templates), [Outlines - Quickstart](https://outlines-dev.github.io/outlines/quickstart/), [LMQL - Overview](https://lmql.ai/docs/language/overview.html), [Microsoft Guidance](https://github.com/microsoft/guidance), [Guidance](https://github.com/microsoft/guidance), [Outlines](https://github.com/normal-computing/outlines) | **2020**: [FLIN: A Flexible Natural Language Interface for Web Navigation](https://arxiv.org/abs/2010.12844v2) <br>**2020**: [Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification](https://arxiv.org/abs/2010.13641v1) <br>**2023**: [CoEdIT: Text Editing by Task-Specific Instruction Tuning](https://arxiv.org/abs/2305.09857) | []

* **Retrieval Augmented Generation (RAG)** 

| Topic                                                    | Resources                                                                                                                      | Papers           | Practices 
|----------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| Building a Vector Database                               |                                                                                                                                 |  |          
|   - Document Loaders, Text Splitters                  | [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/), [LlamaIndex Data Connectors](https://gpt-index.readthedocs.io/en/latest/guides/primer/data_connectors.html) |  |  [] 
|   - Embedding Models                                    | [Sentence Transformers Library](https://www.sbert.net/), [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard), [InferSent](https://github.com/facebookresearch/InferSent)          |  |  [] 
|   - Vector Databases (Chroma, Pinecone, Milvus, FAISS, Annoy) | [Chroma](https://www.trychroma.com/), [Pinecone](https://www.pinecone.io/), [Milvus](https://milvus.io/), [FAISS](https://faiss.ai/), [Annoy](https://github.com/spotify/annoy)  | |  [] 
| RAG Pipelines & Techniques                             |                                                                                                                                 | |           
|   - Orchestrators (LangChain, LlamaIndex, FastRAG)       | [LangChain](https://python.langchain.com/), [LlamaIndex](https://docs.llamaindex.ai/en/stable/), [FastRAG](https://github.com/IntelLabs/fastRAG), [🦜🔗 Awesome LangChain](https://github.com/kyrolabs/awesome-langchain) | |  [] 
|   - Query Expansion, Re-ranking, HyDE                    | [HyDE](https://arxiv.org/abs/2212.10496), [LangChain Retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers/)                              |**2023**: [Query Expansion by Prompting Large Language Models](https://arxiv.org/abs/2305.03653) |  [] 
|   - RAG Fusion                                           | [RAG-fusion](https://github.com/Raudaschl/rag-fusion)                                                                         | |  [] 
|   - Evaluation (Context Precision/Recall, Faithfulness, Relevancy, Ragas, DeepEval) | [Ragas](https://github.com/explodinggradients/ragas/tree/main), [DeepEval](https://github.com/confident-ai/deepeval)                | |  [] 
| Advanced RAG                                               |                                                                                                                                 |  |          
|   - Query Construction (SQL, Cypher)                     | [LangChain Query Construction](https://blog.langchain.dev/query-construction/), [LangChain SQL](https://python.langchain.com/docs/use_cases/qa_structured/sql) | **2022**: [Inferring Implicit Relations in Complex Questions with Language Models](https://arxiv.org/abs/2204.13778v2) | [] 
|   - Agents & Tools (Google Search, Wikipedia, Python, Jira) | [LangChain Agents](https://python.langchain.com/docs/modules/agents/)                                                                | **2022**: [LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models](https://arxiv.org/abs/2212.04088v3) <br>**2022**: [ProgPrompt: Generating Situated Robot Task Plans using Large Language Models](https://arxiv.org/abs/2209.11302v1) <br>**2022**: [Promptagator: Few-shot Dense Retrieval From 8 Examples](https://arxiv.org/abs/2209.11755v1)<br>**2023**: [ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/abs/2303.09014v1)<br>**2023**: [MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting](https://arxiv.org/abs/2305.16896v1)  | [] 
|   - Programmatic LLMs (DSPy)                           | [DSPy](https://github.com/stanfordnlp/dspy), [dspy](https://github.com/stanfordnlp/dspy)                                                                                    | **2023**: [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)<br>**2024**: [AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls](https://arxiv.org/abs/2402.04253) | []

**Chapter 5: Multimodal Learning & Applications**

| Topic                                                    | Resources                                                                                                                    | Papers           | Practices 
|----------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| Multimodal LLMs (CLIP, ViT, LLaVA, MiniCPM-V, GPT-SoVITS) | [OpenAI CLIP](https://openai.com/research/clip), [Google AI Blog: ViT](https://ai.googleblog.com/2020/10/an-image-is-worth-16x16-words.html), [LLaVA](https://llava-vl.github.io/), [MiniCPM-V 2.6](https://github.com/OpenBMB/MiniCPM-V), [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS)  |  **2021**: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020v1) <br> **2022**: [Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language](https://arxiv.org/abs/2204.00598v2) <br> **2023**: [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/abs/2303.17580v4) <br> **2023**: [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381v1)<br>**2023**: [LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents](https://arxiv.org/abs/2311.05437v1) <br> **2024**: [LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model](https://arxiv.org/abs/2401.02330v2) <br> **2024**: [MiniCPM-V: A GPT-4V Level MLLM on Your Phone](https://arxiv.org/abs/2408.01800v1) <br> **2024**: [MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices](https://arxiv.org/abs/2312.16886v2) |  [] 
| Vision-Language Tasks (Image Captioning, VQA, Visual Reasoning) | [Hugging Face: Vision-Language Tasks](https://huggingface.co/docs/transformers/tasks/vision-language-modeling), [Microsoft Kosmos-1](https://arxiv.org/abs/2302.14045), [Google PaLM-E](https://palm-e.github.io/), [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)                      | **2023**: [MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478v3) |  [] 
| Text-to-Image Generation, Video Understanding           | [Stability AI: Stable Diffusion](https://stability.ai/stable-image), [OpenAI DALL-E 2](https://openai.com/dall-e-2), [Hugging Face: Video Understanding](https://huggingface.co/docs/transformers/tasks/video-classification), [Deep-Live-Cam](https://github.com/hacksider/Deep-Live-Cam)  | **2022**: [MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge](https://arxiv.org/abs/2206.08853v2) <br>**2024**: [MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World](https://arxiv.org/abs/2401.08577) |  [] 

**Chapter 6: Deployment & Productionizing LLMs**

* **Deployment Strategies**

| Topic                                                             | Resources                                                                                                                                            | Papers           | Practices 
|------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| Local Servers (LM Studio, Ollama, Oobabooga, Kobold.cpp)          | [LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.ai/), [oobabooga](https://github.com/oobabooga/text-generation-webui), [kobold.cpp](https://github.com/LostRuins/koboldcpp), [llama.cpp](https://github.com/ggerganov/llama.cpp), [mistral.rs](https://github.com/EricLBuehler/mistral.rs), [Serge](https://github.com/serge-chat/serge) | |  [] 
| Cloud Deployment (AWS, GCP, Azure, SkyPilot, Specialized Hardware (TPUs)) | [SkyPilot](https://github.com/skypilot-org/skypilot), [Hugging Face Inference API](https://huggingface.co/inference-api), [Together AI](https://www.together.ai/), [Modal](https://modal.com/docs/guide/ex/potus_speech_qanda), [Metal](https://getmetal.io/)           |  |  [] 
| Serverless Functions, Edge Deployment (MLC LLM, mnn-llm)          | [AWS Lambda](https://aws.amazon.com/lambda/), [Google Cloud Functions](https://cloud.google.com/functions), [Azure Functions](https://azure.microsoft.com/en-us/services/functions/), [MLC LLM](https://github.com/mlc-ai/mlc-llm), [mnn-llm](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md)  |  **2021**: [FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning](https://arxiv.org/abs/2108.06098v3) |  [] 
| LLM Serving                                                        | [LitServe](https://github.com/Lightning-AI/LitServe), [vLLM](https://github.com/vllm-project/vllm), [TGI](https://huggingface.co/docs/text-generation-inference/en/index), [FastChat](https://github.com/lm-sys/FastChat), [Jina](https://github.com/jina-ai/langchain-serve), [LangServe](https://github.com/langchain-ai/langserve)                                             |  **2023**: [S-LoRA: Serving Thousands of Concurrent LoRA Adapters](https://arxiv.org/abs/2311.03285v2) |           |

* **Inference Optimization** 

| Topic                                              | Resources                                                                                                                                                                                   | Papers           | Practices 
|---------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| Quantization (GPTQ, EXL2, GGUF, llama.cpp, exllama) | [Introduction to Quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html), [Quantization with GGUF and llama.cpp Notebook](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing), [4-bit LLM Quantization with GPTQ](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html), [4-bit Quantization using GPTQ Notebook](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing), [ExLlamaV2: The Fastest Library to Run LLMs](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html), [ExLlamaV2 Notebook](https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing), [AutoQuant Notebook](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing), [exllama](https://github.com/turboderp/exllama)  | **2023**: [SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot](https://arxiv.org/abs/2301.00774v3) <br>**2023**: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314v1) <br> **2024**: [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659v4) | []  
| Flash Attention, Key-Value Cache (MQA, GQA)       | [Flash-Attention](https://github.com/Dao-AILab/flash-attention), [Multi-Query Attention](https://arxiv.org/abs/1911.02150), [Grouped-Query Attention](https://arxiv.org/abs/2305.13245)      | **2022**: [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135v2) <br>**2024**: [Prompt Cache: Modular Attention Reuse for Low-Latency Inference](https://arxiv.org/abs/2311.04934v2)  | []
| Knowledge Distillation, Pruning                   | [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531), [To prune, or not to prune: exploring the efficacy of pruning for model compression](https://arxiv.org/abs/1710.01878) | **2021**: [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499v2)  |  [] 
| Speculative Decoding                               | [Hugging Face: Assisted Generation](https://huggingface.co/blog/assisted-generation)                                                                                                     | |  [] 

* **Building with LLMs**

| Topic                                                     | Resources                                                                                                                                             | Papers           | Practices 
|----------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| APIs (OpenAI, Google, Anthropic, Cohere, OpenRouter, Hugging Face) | [OpenAI API](https://platform.openai.com/), [Google AI Platform](https://cloud.google.com/ai-platform/), [Anthropic API](https://docs.anthropic.com/claude/reference/getting-started-with-the-api), [Cohere API](https://docs.cohere.com/docs), [OpenRouter](https://openrouter.ai/), [Hugging Face Inference API](https://huggingface.co/inference-api), [GPTRouter](https://gpt-router.writesonic.com/) |  |  [] 
| Web Frameworks (Gradio, Streamlit)                        | [Gradio](https://www.gradio.app/), [Streamlit](https://docs.streamlit.io/), [ZeroSpace Notebook](https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC)                          | |  [] 
| User Interfaces, Chatbots                               | [Chainlit](https://docs.chainlit.io/overview), [Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat), [llm-ui](https://github.com/llm-ui-kit/llm-ui)                               | |  [] 
| End-to-End LLM Projects |  [Awesome NLP Projects](https://github.com/EugeniuCostezki/awesome-nlp-projects)  | | [] 
| LLM Application Frameworks | [LangChain](https://github.com/hwchase17/langchain), [Haystack](https://haystack.deepset.ai/), [Semantic Kernel](https://github.com/microsoft/semantic-kernel), [LlamaIndex](https://github.com/jerryjliu/llama_index), [LMQL](https://lmql.ai), [ModelFusion](https://github.com/lgrammel/modelfusion), [Flappy](https://github.com/pleisto/flappy), [LiteChain](https://github.com/rogeriochaves/litechain), [magentic](https://github.com/jackmpcollins/magentic) | **2023**: [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/abs/2303.17580v4) <br> **2023**: [MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks](https://arxiv.org/abs/2304.14979v1) <br> **2023**: [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework](https://arxiv.org/abs/2308.00352v5)<br>**2024**: [AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System](https://arxiv.org/abs/2402.15538v1) <br>**2024**: [AUTOACT: Automatic Agent Learning from Scratch via Self-Planning](https://arxiv.org/abs/2401.05268v2) <br> **2024**: [Agents: An Open-source Framework for Autonomous Language Agents](https://arxiv.org/abs/2309.07870v3) <br>**2024**: [MindAgent: Emergent Gaming Interaction](https://arxiv.org/abs/2309.09971v2) <br>**2024**: [AutoAgents: A Framework for Automatic Agent Generation](https://arxiv.org/abs/2309.17288v2) <br> **2024**: [KwaiAgents: Generalized Information-seeking Agent System with Large Language Models](https://arxiv.org/abs/2312.04889v3) <br>**2024**: [TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON](https://arxiv.org/abs/2407.15734v1) <br> **2024**: [AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology](https://arxiv.org/abs/2406.11912v2) <br>**2024**: [OS-Copilot: Towards Generalist Computer Agents with Self-Improvement](https://arxiv.org/abs/2402.07456) <br> **2024**: [Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents](https://arxiv.org/abs/2405.02957v1) | [] 

* **MLOps for LLMs**

| Topic                                          | Resources                                                                                                                          | Papers           | Practices 
|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| CI/CD, Monitoring, Model Management            | [CometLLM](https://github.com/comet-ml/comet-llm), [MLflow](https://mlflow.org/), [Kubeflow](https://www.kubeflow.org/), [Evidently](https://github.com/evidentlyai/evidently), [Arthur Shield](https://www.arthur.ai/get-started),  [Mona](https://github.com/monalabs/mona-openai), [Openllmetry](https://github.com/traceloop/openllmetry), [Graphsignal](https://graphsignal.com/), [Arize-Phoenix](https://phoenix.arize.com/)                |  |  [] 
| Experiment Tracking, Model Versioning          | [Weights & Biases](https://wandb.ai/site/solutions/llmops), [MLflow Tracking](https://www.mlflow.org/docs/latest/tracking.html) |  |  [] 
| Data & Model Pipelines                       | [ZenML](https://zenml.io/), [DVC](https://dvc.org/)                                                                                   |  |  [] 

* **LLM Security**

| Topic                                             | Resources                                                                                                                                  | Papers           | Practices 
|---------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|-----------|-----------|
| Prompt Hacking (Injection, Leaking, Jailbreaking) | [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/), [Prompt Injection Primer](https://github.com/jthack/PIPE), [Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security) |  **2022**: [Ignore Previous Prompt: Attack Techniques For Language Models](https://arxiv.org/abs/2211.09527v1) <br> **2023**: [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://arxiv.org/abs/2305.13860v1) <br> **2024**: [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/2401.06373v1)<br>**2024**: [Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender](https://arxiv.org/abs/2401.06561v1) | [] 
| Backdoors (Data Poisoning, Trigger Backdoors)      | [Trojaning Language Models for Fun and Profit](https://arxiv.org/abs/2008.00313), [Hidden Trigger Backdoor Attacks](https://arxiv.org/abs/1912.02257) | **2024**: [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566v3)  |  [] 
| Defensive Measures (Red Teaming, Garak, Langfuse) | [Red Teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming), [garak](https://github.com/leondz/garak/), [Langfuse](https://github.com/langfuse/langfuse) | **2024**: [HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback](https://arxiv.org/abs/2403.08309v2) <br> **2024**: [Efficient Detection of Toxic Prompts in Large Language Models](https://arxiv.org/abs/2408.11727v1)  |  [] 

**Chapter 7: Research and Applications of LLMs**

| Topic                              | Papers                                                                                                                                                                                    |
|------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| LLM-Driven Research              | **2022**: [From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams](https://arxiv.org/abs/2206.05442v7) <br> **2023**: [ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing](https://arxiv.org/abs/2306.00622v1) <br> **2023**: [ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing](https://arxiv.org/abs/2306.00622) <br>**2023**: [Can large language models provide useful feedback on research papers? A large-scale empirical analysis](https://arxiv.org/abs/2310.01783) <br>**2023**: [Can large language models provide useful feedback on research papers? A large-scale empirical analysis](https://arxiv.org/abs/2310.01783v1)<br>**2024**: [ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models](https://arxiv.org/abs/2404.07738) <br> **2024**: [Autonomous LLM-driven research from data to human-verifiable research papers](https://arxiv.org/abs/2404.17605v1) <br>**2024**: [GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study](https://arxiv.org/abs/2307.05492) <br> **2024**: [OpenResearcher: Unleashing AI for Accelerated Scientific Research](https://arxiv.org/abs/2408.06941v1) <br> **2024**: [The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4](https://arxiv.org/abs/2311.07361v2)|
| LLM for Specific Domains       | **2021**: [The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants](https://arxiv.org/abs/2308.16884) <br> **2023**: [Evaluating ChatGPT text-mining of clinical records for obesity monitoring](https://arxiv.org/abs/2308.01666v1) <br> **2023**: [Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature](https://arxiv.org/abs/2310.16146v1) <br>**2023**: [NoteChat: A Dataset of Synthetic Doctor-Patient Conversations Conditioned on Clinical Notes](https://arxiv.org/abs/2310.15959v2)<br> **2023**: [Towards Expert-Level Medical Question Answering with Large Language Models](https://arxiv.org/abs/2305.09617) <br>**2024**: [EHRTutor: Enhancing Patient Understanding of Discharge Instructions](https://arxiv.org/abs/2310.19212v1) <br>**2024**: [PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models](https://arxiv.org/abs/2309.10238v1) <br> **2024**: [A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges](https://arxiv.org/abs/2311.05112v2) <br>**2024**: [From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models](https://arxiv.org/abs/2311.13063v2)<br>**2024**: [Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review](https://arxiv.org/abs/2311.01918v1) <br>**2024**: [LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models](https://arxiv.org/abs/2309.09708v2) <br>**2024**: [A Computational Framework for Behavioral Assessment of LLM Therapists](https://arxiv.org/abs/2401.00820v1)<br>**2024**: [Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review](https://arxiv.org/abs/2401.01519v2)<br>**2024**: [LLaMA Beyond English: An Empirical Study on Language Capability Transfer](https://arxiv.org/abs/2401.01055v2) <br> **2024**: [Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking](https://arxiv.org/abs/2405.04685v1)<br>**2024**: [Performance Comparison of Turkish Language Models](https://arxiv.org/abs/2404.17010v1) <br>**2024**: [Introducing cosmosGPT: Monolingual Training for Turkish Language Models](https://arxiv.org/abs/2404.17336v1) <br>**2024**: [RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://arxiv.org/abs/2401.08406v2) <br>**2024**: [PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval](https://arxiv.org/abs/2402.19273)<br>**2024**: [Apollo: An Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People](https://arxiv.org/abs/2403.03640) <br> **2024**: [SM70: A Large Language Model for Medical Devices](https://arxiv.org/abs/2312.06974v1)|
| LLM for Agents                 | **2022**: [Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents](https://arxiv.org/abs/2201.07207v2) <br>**2022**: [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://arxiv.org/abs/2204.01691v2)<br>**2022**: [LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action](https://arxiv.org/abs/2207.04429v2)<br>**2022**: [Inner Monologue: Embodied Reasoning through Planning with Language Models](https://arxiv.org/abs/2207.05608v1)<br>**2023**: [Collaborating with language models for embodied reasoning](https://arxiv.org/abs/2302.00763v1)<br>**2023**: [Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents](https://arxiv.org/abs/2302.01560v2) <br>**2024**: [From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models](https://arxiv.org/abs/2401.02777v1) <br> **2024**: [Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents](https://arxiv.org/abs/2306.03314) <br> **2024**: [When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm](https://arxiv.org/abs/2306.02552v2) <br> **2024**: [Interactive Language: Talking to Robots in Real Time](https://arxiv.org/abs/2210.06407v1) <br>**2024**: [Octopus v2: On-device language model for super agent](https://arxiv.org/abs/2404.01744)<br>**2024**: [Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents](https://arxiv.org/abs/2305.02412v2)<br>**2024**: [PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits](https://arxiv.org/abs/2305.02547) <br>**2024**: [SPRING: Studying the Paper and Reasoning to Play Games](https://arxiv.org/abs/2305.15486v3)<br> **2024**: [Reasoning with Language Model is Planning with World Model](https://arxiv.org/abs/2305.14992v2) <br> **2024**: [(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts](https://arxiv.org/abs/2405.11804v1) <br>**2024**: [Faithful Persona-based Conversational Dataset Generation with Large Language Models](https://arxiv.org/abs/2312.10007v1) |

